# YOYO v1

YOLO是物体检测领域的重要模型，其重要贡献在于第一次提出了一种端到端的物体检测算法框架，让实时物体检测任务能取得SOTA性能。

该模型由Joseph Redmon提出，并发表于CVPR 2016。YOLO是"You Only Look Once"的缩写，其名称概括了该模型的特点，相对于其它的多阶段物体检测算法，该算法只需要对图像进行一次处理就可以直接输出图像中所包含物体的位置、类别和置信度信息。

# YOLO的设计

YOLO的输入是一张完整的图片，并且采用了固定的图像大小。网络输出时将图像分为$S \times S$个网格（grid cell），如果某个物体的中心点落在一个网格中，则这个网格就负责预测这个物体。每个网格输出$B$个预测框（bunding box），每个bbox包括位置信息（中心点的$X$和$Y$坐标，及宽度$W$和高度$H$）和该bbox包含物体的置信度（Confidence），$B$个bbox共享一组$C$个物体类别的分类概率。因此，网络输出将是一个$S \times S \times ( 5 \times B + C)$的张量。在论文中输入图片的大小为$448 \times 448$,$S$取$7$，$B$取$2$，使用的PASCAL VOC数据集共包含$20$个物体类别，因此$C$为$20$。

可以看出YOLO的精妙之处在于其通过将图片分成若干个grid cell，将图像检测任务转换成了一个单独的回归任务。针对每个cell，网络同时输出预测框的位置信息和分类概率，不再像两阶段的检测方法那样，预测框选择使用一个模型，分类使用另一个模型。

## 网络结构设计

YOLO的网络结构借鉴了GoogLeNet，用$1 \times 1$卷积层接$3 \times 3$的卷积层替换了GoogLeNet的Inception 模块。YOLO网络共包含24个卷积层和2个全连接层，卷积层用来提取图像特征（其中$1\times 1$卷积层的存在是为了跨通道信息整合、降低通道数目），全连接层用来预测图像位置和类别概率值。最后一个全连接层输出为一个1470维的向量，最后被reshape成一个$7 \times 7 \times 30$大小的张量。

![](2022-09-29-yolo_v1/image_WiSIjCxU1A.png)

### 网络输出

网络输出是一个$S \times S \times ( 5 \times B + C)$的张量，按论文中超参数的取值，则是一个$7 \times 7 \times 30$的张量，对应于49个cell，每个cell有30维的输出。每个cell的前10维是两个bbox的信息，分别为中心坐标$x,y$及宽高$w,h$,这四个值都是归一化值，其中$x,y$是相对于cell左上角的归一化值，$w,h$是相对于整个图片大小的归一化值，每个bbox信息的最后一维是该bbox包含物体的概率$Pr(Object)$。每个cell的后20维是20个类别的概率值，这里的概率是一个条件概率，即在该cell包含物体的条件下的类别概率$Pr(Class_i|Object)$。

![](2022-09-29-yolo_v1/image_54EZGH1iMF.png)

根据bbox包含物体的概率$Pr(Object)$和对应cell的类别条件概率$Pr(Class_i|Object)$,可以计算出每个bbox预测的物体类别概率$Pr(Class_i)$。

$$
Pr(Class_i) = Pr(Object) \times Pr(Class_i|Object)
$$

## 预测过程

在预测阶段，输入图片至网络，在将网络的输出进行后处理，输出最终的bunding box。后处理主要是对输出使用非极大值拟制（NMS），对bbox进行过滤，使每个grid cell最多只留下一个bbox。

### 后处理过程

预测阶段的后处理过程如下：

1. 对于每个cell，选择概率最大的类别$i = \mathop{\arg\max}\limits_{i}{Pr(Class_i | Object)}$及概率$Pr(Class_{i_{max}}|Object)$则可以形成一个$7 \times 7$的Probalility Map；

2. 选择Confidence最大的bbox（在预测时Cofidence和$Pr(Object)$相等）；

3. 使用第1步的$Pr(Class_{i_{max}}|Object)$第2步选中bbox对应的$Pr(Object)$计算$Pr(Class_i)$；

4. 将$Pr(Class_i)$大于阈值的bbox位置信息和类别信息$i$作为最终的检测输出。

![](2022-09-29-yolo_v1/image_-llkV58wjk.png)

## 训练过程

为了取得好的训练效果，YOLO使用ImageNet 1K数据先进行预训练。预训练过程是将网络的前20层加上一个平均池化层和一个全连接层构成一个新的网络，将此网络在ImageNet上训练到一个比较好的准确率（在ImageNet 2012 validation集上取得88%的top-5准确率），将此时前20的权重作为YOLO网络前20的初始化权重。训练时，在已经预训练的20层后添加上4个卷积层和两个全连接层，形成YOLO的最终网络结构，并使用PASCAL VOC数据进行训练。预训练时，输出图片的大小的$224 \times 224$，考虑到物体检测任务中一般需要更多的纹理信息，因此在训练时将输入图片的大小增加至$448 \times 448$。网络中除最后一层使用的是标准ReLU激活函数，其他的层都使用Leaky ReLU激活函数。

### 损失函数

训练的过程中，最重要的无疑是损失函数。YOLO将目标检测看作回归问题（实际上应该是回归和分类问题的结合），为了方便优化，采用了平方和损失函数的形式。

平方和损失函数将定位错误和分类错误赋予等权重并不利于最大化mAP，并且在一张图片中，大部分grid cell并不包含物体，如果这些cell的损失如果等权进行反向传播，将很容易导致包含物体的cell的损失被淹没。为解决这个问题，YOLO的损失函数提高了定位损失的权重，并降低了不包含物体的cell的分类损失的权重。这两个权重使用 $\lambda_{coord}$和$\lambda_{noobj}$来表示，分别取值为$\lambda_{coord} = 5$和$\lambda_{noobj} =0.5$。

使用平方和误差，相对于较小的bbox大的bbox更容易产生较大的定位误差，为了解决这个问题，在YOLO的损失函数中使用宽度和高度的平方跟来代替宽度和高度。

通过以上的设计，损失函数中，不同的bbox对网络预测物体的位置、大小和类别影响不同。最终损失函数的形式如下。

$$
\begin{split}
\mathbb{L}_{YOLO}  &=
\lambda_{obj} \sum^{s^2}_{i=0} \sum^{B}_{j=0}{\mathbb{I}^{obj}_{ij}} \left[ \left( x_i - \hat{x}_i \right)^2 + \left(y_i - \hat{y}_i \right)^2 \right]\\
&+ \lambda_{obj} \sum^{s^2}_{i=0} \sum^{B}_{j=0}{\mathbb{I}^{obj}_{ij}} \left[ \left( \sqrt{w_i} - \sqrt{\hat{w}_i} \right)^2 + \left(\sqrt{h}_i - \sqrt{\hat{h}_i} \right)^2 \right]\\
&+ \lambda_{obj} \sum^{s^2}_{i=0} \sum^{B}_{j=0}{\mathbb{I}^{obj}_{ij}} (C_i - \hat{C}_i)^2 \\
&+ \lambda_{noobj} \sum^{s^2}_{i=0} \sum^{B}_{j=0}{\mathbb{I}^{noobj}_{ij}} (C_i - \hat{C}_i)^2 \\
&+ \sum^{s^2}_{i=0}{\mathbb{I}^{obj}_i} \sum_{c \in classes} (p_i(c) - \hat{p}_i(c))^2
\end{split}


$$

每个grid cell会预测出多个bbox，而在训练中，只希望一个bbox对其对应的grid cell“负责”，这就要使用NMS来进行bbox的过滤。训练过程中，有标注数据作为参考，因此其NMS方法和预测中的过程不同，原则就是选择和标注bbox最相似的bbox，这种相似度使用IOU来进行度量。

公式中，$\mathbb{I}^{obj}_{i}$表示有某个标注的物体中心落在了第i个cell中，$\mathbb{I}^{obj}_{ij}$表示有某个标注的物体中心落在了第i个cell中，且该cell的第j个bbox被选中为该cell“负责”，$\mathbb{I}^{noobj}_{ij}$表示第i个cell未选中的bbox，即$\mathbb{I}^{noobj}_{ij} = 1-\mathbb{I}^{obj}_{ij}$；$\hat{C}_i$为第i个cell所有bbox共享的置信度标签，如果存在某个标注bbox的中心点落在此cell中，则$\hat{C}_i = 1$，否则$  \hat{C}_i =  $$0 $**。**

上边的损失函数可以分成三个部分，即定位损失、置信度损失和分类损失，即：

$$
\mathbb{L}_{YOLO} = \mathbb{L}_{coordinate} + \mathbb{L}_{confidence} +  \mathbb{L}_{class}


$$

其中定位损失为：

$$
\begin{split}
\mathbb{L}_{coordinate}  &=
\lambda_{obj} \sum^{s^2}_{i=0} \sum^{B}_{j=0}{\mathbb{I}^{obj}_{ij}} \left[ \left( x_i - \hat{x}_i \right)^2 + \left(y_i - \hat{y}_i \right)^2 \right]\\
&+ \lambda_{obj} \sum^{s^2}_{i=0} \sum^{B}_{j=0}{\mathbb{I}^{obj}_{ij}} \left[ \left( \sqrt{w_i} - \sqrt{\hat{w}_i} \right)^2 + \left(\sqrt{h}_i - \sqrt{\hat{h}_i} \right)^2 \right]\\
\end{split}


$$

置信度损失为：

$$
\begin{split}
\mathbb{L}_{confidence}  &=
\lambda_{obj} \sum^{s^2}_{i=0} \sum^{B}_{j=0}{\mathbb{I}^{obj}_{ij}} (C_i - \hat{C}_i)^2 \\
&+ \lambda_{noobj} \sum^{s^2}_{i=0} \sum^{B}_{j=0}{\mathbb{I}^{noobj}_{ij}} (C_i - \hat{C}_i)^2
\end{split}

$$

分类损失为：

$$
\begin{split}
\mathbb{L}_{class}  &=
\sum^{s^2}_{i=0}{\mathbb{I}^{obj}_i} \sum_{c \in classes} (p_i(c) - \hat{p}_i(c))^2
\end{split}

$$

单独看每一部分的损失，容易发现，定位损失只受NMS过滤后的bbox的影响，置信度损失受所有bbox的影响，但是被NMS选中和未选中的bbox权重不同，而分类损失仅受预测到物体的bbox影响。

### 非极大值拟制

预测过程的后处理中对bbox的过滤和训练过程中对bbox的过滤，都需要使用到非及大值拟制（NMS）为每一个cell选择一个bbox，但这两个过程略有不同。

在预测过程中，对于每个cell，其bbox需要满足两个条件，就会被选中并输出。这两个条件为：1）该bbox包含物体的置信度为对应cell所有bbox中最大；2）该bbox预测的物体类别概率中最大的值超过预设的阈值。其中第1个条件可以被认为是预测过程中的非极大值拟制。

$$
b = \mathop{\arg\max}\limits_{b}{Pr_b(Object)}

$$

在训练过程中，每个cell，其bbox也需要满足两个条件，才能被选中，如果第$i$个cell的第$j$个cell被选中，则在损失函数中，对应的$\mathbb{I}^{obj}_{ij}$值为1，否则为0。这两个条件为：1）某个标注的bbox中心点落在了该bbox对应的cell中；2）该bbox与标注bbox的IOU为对应cell所有bbox中最大。其中第2个条件可以被认为是预测过程中的非极大值拟制。

$$
b = \mathop{\arg\max}\limits_{b}{IOU_b}

$$

# YOLO的特点

YOLO是一种基于深度神经网络的，检测性能达到SOTA水平，执行速度满足实时监测要求，且容易训练的端到端物体检测算法。较其它的一些物体检测模型，存在诸多优势，同时也存在很大的改进空间。

YOLO的主要优势有：

1. **处理速度快**
   
   由于YOLO为单阶段算法，步骤少，加之模型输出候选的边界框更少，因此其其速度更快，可以达到实时物体检测（每秒处理30帧以上）的要求，其基准模型在Titan X GPU上处理速度可以达到45FPS，快速版Fast YOLO可以达到155FPS（mAP较基准版有所下降）；

2. **模型泛化性强**
   
   YOLO框架中，整张图片被整体输入卷积神经网络，使网络可以学习到整张图片的上下文信息，因此相对于R-CNN等只讲特定候选框输入网络的模型而言，其模型泛化性更强。例如，YOLO算法在VOC数据集上训练，在Picasso和Peple-Art数据集上测试仍然有较好的性能；

3. **易于实现**
   
   YOLO是一个统一的算法框架，进需要一个端到端的深度神经网络就可以实现所有物体检测任务的工作流，相比于DPM和R-CNN等多阶段的物体检测算法，实现和训练过程更加简单。

YOLO利用了深度卷积神经网络强大的学习能力，同时又是端到端模型，有利于算法各个阶段统一优化，性能可达到SOTA水平，但mAP逊于Fast R-CNN等模型。由于YOLO是将整张图片分成了若干个grid cell，为了减少计算量，grid cell的数量并不会设置的特别大，例如在论文中，就只有49个grid cell。因此无法检测超过grid cell数量的物体，且于较小的物体，grid cell就显得分辨率太大，影响对小物体检测的准确率，这也不利于图像定位。分析YOLO和Fast R-CNN模型的错误类型可以发现，YOLO的定位错误大于其它类型的错误的总和，而Fast R-CNN模型的特点是背景识别错误更多。

# 参考资料

\[1][ You Only Look Once: Unified, Real-Time Object Detection](http://arxiv.org/abs/1506.02640 " You Only Look Once: Unified, Real-Time Object Detection")，[Joseph Redmon](https://pjreddie.com/static/Redmon%20Resume.pdf "Joseph Redmon")等；

\[2] [【Yolo v1】原理与实现 | 目标检测 | 图文详解 | 提供Pytorch代码可直接运行](https://zhuanlan.zhihu.com/p/431973369 "【Yolo v1】原理与实现 | 目标检测 | 图文详解 | 提供Pytorch代码可直接运行")，[wenjtop](https://www.zhihu.com/people/wenjtop "wenjtop")；

\[3] [GitHub - Lxrd-AJ/YOLO\_V1: You Only Look Once version 1](https://github.com/Lxrd-AJ/yolo_v1 "GitHub - Lxrd-AJ/YOLO_V1: You Only Look Once version 1")，[Lxrd-AJ](https://github.com/Lxrd-AJ "Lxrd-AJ")；

\[4][【精读AI论文】YOLO V1目标检测，看我就够了](https://www.bilibili.com/video/BV15w411Z7LG/?spm_id_from=333.337.search-card.all.click "【精读AI论文】YOLO V1目标检测，看我就够了")，[同济子豪兄](https://space.bilibili.com/1900783 "同济子豪兄")。
